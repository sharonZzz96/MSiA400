---
title: "lab2-Yueying Zhang"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
redwine = read.table('redwine.txt', sep='\t', header=TRUE)
```

# Problem 1
```{r}
# calculate average of RS and SD
rs_mean <- mean(redwine$RS, na.rm=TRUE)
sd_mean <- mean(redwine$SD, na.rm=TRUE)
print(paste0('Average for RS: ', rs_mean))
print(paste0('Average for SD: ', sd_mean))
```

# Problem 2
```{r}
# simple linear regression of SD and FS
sd_vector <- as.vector(redwine[!is.na(redwine$SD), 'SD'])
fs_vector <- as.vector(redwine[!is.na(redwine$SD), 'FS'])
fit1 <- lm(sd_vector~fs_vector)
print(coefficients(fit1))
```

# Problem 3
```{r}
# caculated fitted value for missing value in SD
fs_17 <- as.vector(redwine[is.na(redwine$SD), 'FS'])
sd_missing <- coefficients(fit1)[1] + coefficients(fit1)[2] * fs_17
# impute missing SD
redwine$SD[which(is.na(redwine$SD))] <- sd_missing
# average of SD
sd_mean2 <- mean(redwine$SD)
print(paste0('Average of SD after imputation: ', sd_mean2))
```

# Problem 4
```{r}
# replace missing value for RS with its mean
redwine$RS[which(is.na(redwine$RS))] <- rs_mean
rs_mean2 <- mean(redwine$RS)
print(paste0('Average of RS after imputation: ', rs_mean2))
```

# Problem 5
```{r}
# fit multiple linear regression
winemodel <- lm(QA~FA+VA+CA+RS+CH+FS+SD+DE+PH+SU+AL, data=redwine)
print(coefficients(winemodel))
```

# Problem 6
```{r}
summary(winemodel)
```

PH is least likely to be related to QA since it has largest p-value (0.414413).

# Problem 7
```{r}
# 5-fold cross validation
CVInd <- function(n, K) {  #n is sample size; K is number of parts; returns K-length list of indices for each part
   m <- floor(n/K)  #approximate size of each part
   r <- n - m * K  
   I <- sample(n, n)  #random reordering of the indices
   Ind <- list()  #will be list of indices for all K parts
   length(Ind) <- K
   for (k in 1:K) {
      if (k <= r) kpart <- ((m+1)*(k-1)+1):((m+1)*k)  
         else kpart<-((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r))
      Ind[[k]] <- I[kpart]  #indices for kth part of data
   }
   Ind
}

Nrep <- 20 #number of replicates of CV
K <- 5  #K-fold CV on each replicate
n <- nrow(redwine)
y <- redwine$QA
SSE <- matrix(0, Nrep, 1)
for (j in 1:Nrep) {
  Ind <- CVInd(n, K)
  yhat <- y
  for (k in 1:K) {
     winemodel_cv <- lm(QA~FA+VA+CA+RS+CH+FS+SD+DE+PH+SU+AL,
                        data=redwine[-Ind[[k]],])
     yhat[Ind[[k]]] <- as.numeric(predict(winemodel_cv, redwine[Ind[[k]],]))
  } #end of k loop
  SSE[j,] <- c(sum((y-yhat)^2))
} #end of j loop
print(paste0('Average SSE: ', apply(SSE, 2, mean)))
```

# Problem 8
```{r}
# identify and remove outliers in PH
ph_mean <- mean(redwine$PH)
ph_sd <- sd(redwine$PH)
upper <- ph_mean + 3 * ph_sd
lower <- ph_mean - 3 * ph_sd
redwine2 <- redwine[which(redwine$PH>=lower & redwine$PH<=upper),]
print(paste0('Mean: ', ph_mean))
print(paste0('SD: ', ph_sd))
print(paste0('Previous dimension: ', dim(redwine)))
print(paste0('New dimension: ', dim(redwine2)))
print(paste0('Row numbers after deleting outliers: ', nrow(redwine2)))
print(paste0('Rows removed: ', nrow(redwine)-nrow(redwine2)))
```

# Problem 9
```{r}
winemodel2 <- lm(QA~FA+VA+CA+RS+CH+FS+SD+DE+PH+SU+AL, data=redwine2)
summary(winemodel2)
```

Model 2 with outlier removed has larger F-statistics and adjusted R-squared, so I would prefer model 2.

VA, CH, SD, SU, AL are most likely to correlate with QA.